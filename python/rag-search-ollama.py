"""
uv add \
  faiss-cpu>=1.11.0 \
  fastapi>=0.115.14 \
  langchain>=0.3.26 \
  langchain-community>=0.3.27 \
  langchain-ollama>=0.3.3 \
  ollama>=0.5.1 \
  openpyxl>=3.1.5 \
  pandas>=2.3.0 \
  pymupdf>=1.26.3 \
  pypdf>=5.7.0 \
  python-docx>=1.2.0 \
  ruff>=0.12.2 \
  tiktoken>=0.9.0 \
  unstructured>=0.18.3 \
  uvicorn>=0.35.0
"""

import os
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import (
    TextLoader,
    DirectoryLoader,
    PyPDFLoader,
    UnstructuredWordDocumentLoader,
)
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from openpyxl import load_workbook


# è®¾ç½® Ollama çš„ä¸»æœºåœ°å€
OLLAMA_HOST = "http://xxx:11434"
os.environ["OLLAMA_HOST"] = OLLAMA_HOST
app = FastAPI(title="RAG QA API")


# æ–‡æ¡£ç›®å½•
DOCS_DIR = "docs"
INDEX_DIR = "FAISS_INDEX"


def load_excel():
    # åŠ è½½ excel æ–‡ä»¶
    filepath = "xx.xlsx"
    sheets = ["0307", "0328æ›´æ–°"]
    data = []
    for sheet in sheets:
        d = read_excel_fill_merged("docs/" + filepath, sheet)
        data.extend(d)
    return rows_to_documents(data=data, sheet_name=filepath)


def rows_to_documents(data, sheet_name):
    documents = []
    headers = data[0]  # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜

    for idx, row in enumerate(data[1:], start=2):  # ä»ç¬¬äºŒè¡Œå¼€å§‹ï¼Œè¡Œå·ä»2èµ·
        # æ‹¼æ¥æˆâ€œæ ‡é¢˜ï¼šå†…å®¹ï¼›æ ‡é¢˜ï¼šå†…å®¹ï¼›...â€æ ¼å¼çš„å­—ç¬¦ä¸²
        parts = []
        for header, cell in zip(headers, row):
            if cell is None:
                cell = ""
            parts.append(f"{header}ï¼š{cell}")
        text = "ï¼›".join(parts)
        metadata = {"sheet": sheet_name, "row": idx}
        documents.append(Document(page_content=text, metadata=metadata))

    return documents


def read_excel_fill_merged(filepath, sheet_name):
    wb = load_workbook(filename=filepath, data_only=True)
    ws = wb[sheet_name]

    data = []
    for row in ws.iter_rows(values_only=True):
        # æŠŠNoneæ›¿æ¢æˆ''ï¼Œè½¬æ¢æˆåˆ—è¡¨
        clean_row = [cell if cell is not None else "" for cell in row]
        data.append(clean_row)

    merged_cells = ws.merged_cells.ranges

    for merged_range in merged_cells:
        min_row, min_col, max_row, max_col = (
            merged_range.min_row - 1,
            merged_range.min_col - 1,
            merged_range.max_row - 1,
            merged_range.max_col - 1,
        )
        fill_value = data[min_row][min_col]
        for r in range(min_row, max_row + 1):
            for c in range(min_col, max_col + 1):
                data[r][c] = fill_value

    return data


def load_all_docs(directory):
    all_docs = []

    # åŠ è½½ txt æ–‡ä»¶
    txt_loader = DirectoryLoader(directory, glob="**/*.txt", loader_cls=TextLoader)
    all_docs.extend(txt_loader.load())

    # åŠ è½½ pdf æ–‡ä»¶
    pdf_loader = DirectoryLoader(directory, glob="**/*.pdf", loader_cls=PyPDFLoader)
    all_docs.extend(pdf_loader.load())

    # åŠ è½½ docx æ–‡ä»¶
    word_loader = DirectoryLoader(
        directory, glob="**/*.docx", loader_cls=UnstructuredWordDocumentLoader
    )
    all_docs.extend(word_loader.load())

    # åŠ è½½ excel æ–‡ä»¶
    all_docs.extend(load_excel())
    return all_docs


# 1. åŠ è½½æ–‡æ¡£
# loader = TextLoader("docs/my_knowledge.txt", encoding="utf-8")
# documents = loader.load()
# loader = DirectoryLoader(DOCS_DIR, glob="**/*.txt", loader_cls=TextLoader)

documents = load_all_docs(DOCS_DIR)

# 2. åˆ†å‰²æ–‡æ¡£
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# 3. åˆ›å»º embedding å¹¶æ„å»ºå‘é‡ç´¢å¼•
# model: bge-m3, nomic-embed-text
embedding = OllamaEmbeddings(model="bge-m3", base_url=OLLAMA_HOST)
vectorstore = FAISS.from_documents(docs, embedding)
# ä¿å­˜åˆ°ç¡¬ç›˜
# vectorstore.save_local(INDEX_DIR)

# 4. åˆå§‹åŒ– LLM
# model: deepseek-r1:1.5b,  qwen2.5:latest deepseek-r1:8b
llm = OllamaLLM(model="qwen2.5:latest", base_url=OLLAMA_HOST)

# 5. åˆ›å»º RAG QA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, retriever=vectorstore.as_retriever(), return_source_documents=True
)

# 6. ç”¨æˆ·æŸ¥è¯¢
# query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š\n> ")

# 7. è·å–å›ç­”
# result = qa_chain.invoke({"query": "è¯´ä¸€è¯´å¤šåª’ä½“"})

# 8. æ‰“å°ç»“æœ
# print("\nğŸ§  å›ç­”ï¼š\n", result["result"])

# print("\nğŸ“š ç›¸å…³ç‰‡æ®µï¼š")
# for i, doc in enumerate(result["source_documents"]):
#     print(f"\n--- ç‰‡æ®µ {i+1} ---\n{doc.page_content}")


# å…è®¸çš„è·¨åŸŸæ¥æºåˆ—è¡¨ï¼Œ* ä»£è¡¨æ‰€æœ‰
origins = [
    "*",  # å…è®¸æ‰€æœ‰åŸŸåè·¨åŸŸï¼Œç”Ÿäº§ç¯å¢ƒå¯æ”¹æˆæŒ‡å®šåŸŸå
    # "http://localhost",
    # "http://localhost:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # å…è®¸è®¿é—®çš„åŸŸååˆ—è¡¨
    allow_credentials=True,  # æ˜¯å¦å…è®¸æºå¸¦Cookieç­‰å‡­è¯
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚æ–¹æ³• GET POST PUT DELETE...
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)


class QueryRequest(BaseModel):
    query: str


@app.post("/query")
async def query_qa(req: QueryRequest):
    if not req.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")

    result = qa_chain.invoke({"query": req.query})
    return {
        "answer": result["result"],
        # "source_documents": [
        #     {"page_content": doc.page_content} for doc in result["source_documents"]
        # ],
        # "result": result,
    }
